[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistics with R",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/01_autoregressive_time_series.html",
    "href": "courses/C05 Capstone project/01_autoregressive_time_series.html",
    "title": "2  Autoregressive Time Series Models",
    "section": "",
    "text": "2.1 Earth Quake Data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nearthquakes.dat &lt;- read.delim(\"./data/earthquakes.txt\")\nearthquakes.dat$Quakes = as.numeric(earthquakes.dat$Quakes)\n\n\ny.dat=earthquakes.dat$Quakes[1:100] ## this is the training data\ny.new=earthquakes.dat$Quakes[101:103] ## this is the test data",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autoregressive Time Series Models</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/01_autoregressive_time_series.html#model-specification",
    "href": "courses/C05 Capstone project/01_autoregressive_time_series.html#model-specification",
    "title": "2  Autoregressive Time Series Models",
    "section": "2.2 Model Specification",
    "text": "2.2 Model Specification\nAn autoregressive (AR) time series model, of order \\(p\\), abbreviated as AR(\\(p\\)), takes the form: \\[\n\\begin{align}\ny_{t} &= \\phi_{1} y_{t-1} + \\ldots + \\phi_{p} y_{t-p} + \\epsilon_{t}, \\quad \\epsilon_{t} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^{2}) \\\\\n&= \\sum_{j=1}^{p} \\phi_{j} y_{t-j} + \\epsilon_{t}\n\\end{align}\n\\] for \\(t = 1, \\ldots, T\\).\nThe \\(\\phi_{j}\\)s are referred to as the AR coefficients, and \\(\\epsilon_{t}\\)s are known as the innovations. We assume that the \\(\\epsilon_{t}\\)s are conditionally independent on past values of the series. Similarly, we assume that they are independent and identically distributed following a zero-mean normal distribution with variance \\(\\sigma^{2}\\).\nFor any \\(T &gt; p\\) we find the following likelihood: \\[\np(y_{1:T} \\mid \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2}) = p(y_{1:p}) \\ \\prod_{t=p+1}^{T} p(y_{t} \\mid y_{(t-1):(t-p)}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2})\n\\] The lead term, \\(p(y_{1:p})\\), is the joint density of the \\(p\\) initial values. If we assume these values are fixed and known, then we can write for the subsequent values: \\[\np(y_{(p+1):T} \\mid y_{1:p}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2}) = \\prod_{t=p+1}^{T} p(y_{t} \\mid y_{(t-1):(t-p)}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2})\n\\]\nNow, we know that the observations are given by: \\[\n\\begin{align}\ny_{t} &= \\sum_{j=1}^{p} \\phi_{j} y_{t-j} + \\epsilon_{t}, \\quad \\epsilon_{t} \\overset{\\text{iid}}{\\sim}\\mathcal{N}(0, \\sigma^{2}) \\\\\n\\Rightarrow y_{t} &\\sim \\mathcal{N}\\left(\\sum_{j=1}^{p} \\phi_{j} y_{t-j}, \\sigma^{2}\\right)\n\\end{align}\n\\]\nWe now introduce the following vectors: \\[\n\\begin{align}\n\\mathbf{f}_{t} &= \\begin{bmatrix}\ny_{t-1} & y_{t-2} & \\ldots & y_{t-p}\n\\end{bmatrix}^{\\top} \\\\\n\\boldsymbol{\\phi} &= \\begin{bmatrix}\n\\phi_{1} & \\phi_{2} & \\ldots & \\phi_{p}\n\\end{bmatrix}^{\\top} \\\\\n\\end{align}\n\\] We can then write: \\[\ny_{t} \\sim \\mathcal{N}\\left(\\mathbf{f}_{t}^{\\top} \\boldsymbol{\\phi} , \\sigma^{2}\\right)\n\\] so that we have: \\[\n\\begin{align}\np(y_{(p+1):T} \\mid y_{1:p}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2}) &= \\prod_{t=p+1}^{T} p(y_{t} \\mid y_{(t-1):(t-p)}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2}) \\\\\n&= \\prod_{t=p+1}^{T} \\mathcal{N}\\left( \\mathbf{f}_{t}^{\\top} \\boldsymbol{\\phi}, \\sigma^{2} \\right)\n\\end{align}\n\\] We can also combine the \\(n\\) \\(\\mathbf{f}_{t}\\) vectors into an \\(F\\) matrix: \\[\n\\mathbf{F} = \\begin{bmatrix}\ny_{p} & y_{p+1} & \\cdots & y_{T-1} \\\\\ny_{p-1} & y_{p} & \\cdots & y_{T-2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{1} & y_{2} & \\cdots & y_{T-p}\n\\end{bmatrix}\n\\] so that we can write: \\[\n\\begin{align}\np(y_{(p+1):T} \\mid y_{1:p}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2}) &= \\prod_{t=p+1}^{T} p(y_{t} \\mid y_{(t-1):(t-p)}, \\phi_{1}, \\ldots, \\phi_{p}, \\sigma^{2}) \\\\\n&= \\prod_{t=p+1}^{T} \\mathcal{N}\\left( \\mathbf{f}_{t}^{\\top} \\boldsymbol{\\phi}, \\sigma^{2} \\right) \\\\\n&= \\mathcal{N}\\left(\\mathbf{F}^{\\top} \\boldsymbol{\\phi}, \\sigma^{2} \\mathbf{I}\\right)\n\\end{align}\n\\] Notice the similarity between the AR model and the multiple linear regression model:\n\\[\n\\begin{align}\n\\mathbf{y} &= \\begin{bmatrix}\ny_{p+1}, \\ldots, y_{T}\n\\end{bmatrix}^{\\top} \\in \\mathbb{R}^{n} \\\\\n\\mathbf{y} &= \\mathbf{F}^{\\top} \\boldsymbol{\\phi} + \\boldsymbol{\\epsilon}\n\\end{align}\n\\] To complete the model formulation we use a conditional conjugate prior for \\(\\phi_{1}, \\ldots, \\phi_{p}\\) and \\(\\sigma^{2}\\). The conditional conjugate priors are: \\[\n\\begin{align}\np(\\boldsymbol{\\phi}, \\sigma^{2}) &= p(\\boldsymbol{\\phi} \\mid \\sigma^{2})\\ p(\\sigma^{2}) \\\\\n&= \\mathcal{N}\\left( \\boldsymbol{\\phi} \\mid \\boldsymbol{m}_{0}, \\sigma^{2} \\mathbf{C}_{0}\\right) \\mathrm{IG}\\left(\\sigma^{2} \\mid \\frac{n_{0}}{2}, \\frac{d_{0}}{2}\\right)\n\\end{align}\n\\]",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Autoregressive Time Series Models</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/02_AR_model_fitting_example.html",
    "href": "courses/C05 Capstone project/02_AR_model_fitting_example.html",
    "title": "3  AR model fitting example",
    "section": "",
    "text": "3.1 Bayesian conjugate analysis",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AR model fitting example</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/02_AR_model_fitting_example.html#bayesian-conjugate-analysis",
    "href": "courses/C05 Capstone project/02_AR_model_fitting_example.html#bayesian-conjugate-analysis",
    "title": "3  AR model fitting example",
    "section": "",
    "text": "3.1.1 Simulate data\nWe give an example about fitting an AR(2) model using simulated data. We simulate 200 observations from the following model: \\[\ny_{t} = 0.5 y_{t-1} + 0.4 y_{t-2} + \\epsilon_{t}, \\quad \\epsilon_{t} \\sim \\mathcal{N}(0, 1)\n\\]\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.1.0     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\n\n\n\nFigure 3.1: Example time series from a given AR(2) model.\n\n\n\n\n\n\n\n3.1.2 Prior sensitivity analysis\nNow, we perform a prior sensitivity analysis for the choice of hyperparameters in prior distribution. We choose three sets of prior hyperparameters and plot the posterior distribution \\(p(\\phi_{1}, \\phi_{2} \\mid y_{1:n})\\). The three sets of prior hyperparameters are:\n\n\\(\\mathbf{m}_{0} = \\begin{bmatrix}0 & 0\\end{bmatrix}^{\\top}\\), \\(\\mathbf{C}_{0} = \\mathbf{I}_{2}\\), \\(n_{0} = 2\\), and \\(d_{0}=2\\).\n\\(\\mathbf{m}_{0} = \\begin{bmatrix}0 & 0\\end{bmatrix}^{\\top}\\), \\(\\mathbf{C}_{0} = \\mathbf{I}_{2}\\), \\(n_{0} = 6\\), and \\(d_{0}=1\\).\n\\(\\mathbf{m}_{0} = \\begin{bmatrix}-\\frac{1}{2} & -\\frac{1}{2}\\end{bmatrix}^{\\top}\\), \\(\\mathbf{C}_{0} = \\mathbf{I}_{2}\\), \\(n_{0} = 6\\), and \\(d_{0}=1\\).\n\n\n## prior sensitivity analysis\n## plot posterior distribution of phi_1 and phi_2 on a grid \n\nlibrary(colorRamps)\nlibrary(leaflet)\nlibrary(fields)\n\nLoading required package: spam\n\n\nSpam version 2.11-3 (2026-01-05) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\n\n\nAttaching package: 'spam'\n\n\nThe following objects are masked from 'package:base':\n\n    backsolve, forwardsolve\n\n\nLoading required package: viridisLite\n\n\nLoading required package: RColorBrewer\n\n\n\nTry help(fields) to get started.\n\n\n\nAttaching package: 'fields'\n\n\nThe following object is masked from 'package:leaflet':\n\n    addLegend\n\nlibrary(mvtnorm)\n\n\nAttaching package: 'mvtnorm'\n\n\nThe following objects are masked from 'package:spam':\n\n    rmvnorm, rmvt\n\n## generate grid\nN = 100\ncoordinates_1=seq(-3,3,length.out = N)\ncoordinates_2=seq(-3,3,length.out = N)\ncoordinates=expand.grid(coordinates_1,coordinates_2)\ncoordinates=as.matrix(coordinates)\n\n## set up\np=2  ## order of AR process\nn.all=length(y.sample) ## T, total number of data\n\nY=matrix(y.sample[3:n.all],ncol=1)\nFmtx=matrix(c(y.sample[2:(n.all-1)],y.sample[1:(n.all-2)]),nrow=p,byrow=TRUE)\nn=length(Y)\n\n## function to compute parameters for the posterior distribution of phi_1 and phi_2\n## the posterior distribution of phi_1 and phi_2 is a multivariate t distribution\n\ncal_parameters=function(m0=matrix(c(0,0),nrow=2),C0=diag(2),n0,d0){\n  e=Y-t(Fmtx)%*%m0\n  Q=t(Fmtx)%*%C0%*%Fmtx+diag(n)\n  Q.inv=chol2inv(chol(Q))  ## similar as solve, but more robust\n  A=C0%*%Fmtx%*%Q.inv\n  m=m0+A%*%e\n  C=C0-A%*%Q%*%t(A)\n  n.star=n+n0\n  d.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n  \n  params=list()\n  params[[1]]=n.star\n  params[[2]]=d.star\n  params[[3]]=m\n  params[[4]]=C\n  \n  return(params)\n}\n\n\n## evaluate density at the grid points\nget_density=function(param){\n  location=param[[3]]\n  scale=as.numeric(param[[2]]/param[[1]])*param[[4]]\n  density=rep(0,N^2)\n  \n  for (i in 1:N^2) {\n    xi=coordinates[i,]\n    density[i]=dmvt(xi,delta=location,sigma=scale,df=param[[1]])\n  }\n  \n  density_expand=matrix(density,nrow=N)\n  return(density_expand)\n}\n\n## calculate density for three sets of hyperparameters\nparams1=cal_parameters(n0=2,d0=2)\nparams2=cal_parameters(n0=6,d0=1)\nparams3=cal_parameters(m0=matrix(c(-0.5,-0.5),nrow=2),n0=6,d0=1)\n\ncol.list=matlab.like2(N)\nZ=list(get_density(params1),get_density(params2),get_density(params3))\n\nop &lt;- par(mfrow = c(1,3),\n          oma = c(5,4,0,0) + 0.1,\n          mar = c(4,4,0,0) + 0.2)\nimage(coordinates_1,coordinates_2,Z[[1]],col=col.list,\n      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))\nimage(coordinates_1,coordinates_2,Z[[2]],col=col.list,\n      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))\nimage(coordinates_1,coordinates_2,Z[[3]],col=col.list,\n      zlim=range(unlist(Z)),xlab=expression(phi[1]),ylab=expression(phi[2]))\n\n\n\n\n\n\n\n\n\n\n3.1.3 Posterior inference\nWe now sample 5000 sets of (\\(\\phi_{1}\\), \\(\\phi_{2}\\), \\(\\nu\\)) from their marginal posterior distributions and plot them. For prior hyperparameters we take \\(\\mathbf{m}_{0} = \\begin{bmatrix} 0 & 0\\end{bmatrix}^{\\top}\\), \\(\\mathbf{C}_{0} = \\mahtbf{I}_{2}\\), \\(n_{0}=2\\) and \\(d_{0}=2\\).\n\nm0=matrix(rep(0,p),ncol=1)\nC0=diag(p)\nn0=2\nd0=2\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\n\nn.sample=5000\n\nnu.sample=rep(0,n.sample)\nphi.sample=matrix(0,nrow=n.sample,ncol=p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)\n  nu.sample[i]=nu.new\n  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)\n  phi.sample[i,]=phi.new\n}\n\npar(mfrow=c(1,3))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,2]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\n\n\n\n3.1.4 Model checking by in-sample prediction and interval estimation\nTo check whether the model fits well, we plot the posterior point and interval estimate for each point.\n\n## get in sample prediction\npost.pred.y=function(s){\n  \n  beta.cur=matrix(phi.sample[s,],ncol=1)\n  nu.cur=nu.sample[s]\n  mu.y=t(Fmtx)%*%beta.cur\n  sapply(1:length(mu.y), function(k){rnorm(1,mu.y[k],sqrt(nu.cur))})\n  \n  \n}  \n\ny.post.pred.sample=sapply(1:5000, post.pred.y)\n\n## show the result\nsummary.vec95=function(vec){\n  c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))\n}\n\nsummary.y=apply(y.post.pred.sample,MARGIN=1,summary.vec95)\n\nplot(Y,type='b',xlab='Time',ylab='',ylim=c(-7,7),pch=16)\nlines(summary.y[2,],type='b',col='grey',lty=2,pch=4)\nlines(summary.y[1,],type='l',col='purple',lty=3)\nlines(summary.y[3,],type='l',col='purple',lty=3)\nlegend(\"topright\",legend=c('Truth','Mean','95% C.I.'),lty=1:3,col=c('black','grey','purple'),\n       horiz = T,pch=c(16,4,NA))",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AR model fitting example</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/02_AR_model_fitting_example.html#prediction-for-ar-models",
    "href": "courses/C05 Capstone project/02_AR_model_fitting_example.html#prediction-for-ar-models",
    "title": "3  AR model fitting example",
    "section": "3.2 Prediction for AR models",
    "text": "3.2 Prediction for AR models\nWe can use the autoregressive nature of the process to make predictions for future observations \\(y_{t}\\) where \\(t &gt; T\\): \\[\ny_{T+h}^{(s)} \\sim \\mathcal{N}\\left(\\sum_{j=1}^{p} \\phi_{j}^{(s)} y_{T+h-j}^{(s)}, \\nu^{(s)}\\right)\n\\] with \\[\ny_{T+h-j}^{(s)} = \\begin{cases}\ny_{t} & \\text{if } T + h - j \\le T \\\\\ny_{t}^{(s)} &\\text{if } T+h-j &gt; T\n\\end{cases}\n\\]\n\ny_pred_h_step = function(h.step, s) {\n  phi.cur = matrix(phi.sample[s,], ncol = 1)\n  nu.cur = nu.sample[s]\n  y.cur = c(y.sample[200], y.sample[199])\n  y.pred = rep(0, h.step)\n  for (i in 1:h.step) {\n    mu.y = sum(y.cur * phi.cur)\n    y.new = rnorm(1, mu.y, sqrt(nu.cur))\n    y.pred[i] = y.new\n    y.cur = c(y.new, y.cur)\n    y.cur = y.cur[-length(y.cur)]\n  }\n  return(y.pred)\n}\n\ny.post.pred.ahead = sapply(1:5000, function(s){y_pred_h_step(h.step=3, s=s)})\n\npar(mfrow = c(1, 3))\nhist(y.post.pred.ahead[1,], freq=FALSE, xlab=expression(y[201]), main=\"\")\nlines(density(y.post.pred.ahead[1,]), type=\"l\", col=\"red\")\nhist(y.post.pred.ahead[2,], freq=FALSE, xlab=expression(y[202]), main=\"\")\nlines(density(y.post.pred.ahead[2,]), type=\"l\", col=\"red\")\nhist(y.post.pred.ahead[3,], freq=FALSE, xlab=expression(y[203]), main=\"\")\nlines(density(y.post.pred.ahead[3,]), type=\"l\", col=\"red\")\n\n\n\n\n\n\n\n\n\napply(y.post.pred.ahead, MARGIN = 1, summary.vec95)\n\n            [,1]        [,2]        [,3]\n[1,] -1.91254892 -2.07324746 -2.39970722\n[2,]  0.06828065  0.05886658  0.07425939\n[3,]  2.09506189  2.18037841  2.56113887",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AR model fitting example</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/03_first_step_project.html",
    "href": "courses/C05 Capstone project/03_first_step_project.html",
    "title": "4  AR model fitting example",
    "section": "",
    "text": "4.1 Bayesian conjugate analysis",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR model fitting example</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/03_first_step_project.html#bayesian-conjugate-analysis",
    "href": "courses/C05 Capstone project/03_first_step_project.html#bayesian-conjugate-analysis",
    "title": "4  AR model fitting example",
    "section": "",
    "text": "4.1.1 Earthquakes data\nFor the earthquake data, perform a conjugate Bayesian analysis of AR(2) model to the data. Using priors \\(\\mathbf{m}_{0} = \\begin{bmatrix} 0 & 0\\end{bmatrix}^{\\top}\\), \\(\\mathbf{C}_{0} = 10 \\mathbf{I}_{2}\\), and \\(n_{0} = d_{0} = 0.02\\). Simulate 5000 posterior samples of \\(\\nu\\), \\(\\phi_{1}\\), and \\(\\phi_{2}\\) and use these 5000 posterior samples to perform posterior inference.\nWhat is the posterior mean estimate of two AR coefficients and the variance parameter for the noise term?\n\nearthquakes.dat &lt;- read.delim(\"./data/earthquakes.txt\")\nearthquakes.dat$Quakes = as.numeric(earthquakes.dat$Quakes)\n\n\ny.dat=earthquakes.dat$Quakes[1:100] ## this is the training data\ny.new=earthquakes.dat$Quakes[101:103] ## this is the test data\n\nplot.ts(\n  y.dat,\n  ylab = expression(italic(y)[italic(t)]),\n  xlab = expression(italic(t)),\n  main = \"\"\n)\n\n\n\n\n\n\n\n\n\n\n4.1.2 Posterior inference\nWe sample 5000 sets of (\\(\\phi_{1}\\), \\(\\phi_{2}\\), \\(\\nu\\)) from their marginal posterior distributions and plot them.\n\nlibrary(mvtnorm)\n\n## set up\np = 2  ## order of AR process\nn.all = length(y.dat) ## T, total number of data\n\nY = matrix(y.dat[3:n.all], ncol = 1)\nFmtx = matrix(c(y.dat[2:(n.all-1)], y.dat[1:(n.all-2)]), nrow=p, byrow=TRUE)\nn = length(Y)\n\n## define the priors\nm0 = matrix(rep(0,p),ncol=1)\nC0 = diag(p)\nn0 = 0.02\nd0 = 0.02\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\n\n## sample from joint posterior distribution\nn.sample = 5000\n\nnu.sample = rep(0, n.sample)\nphi.sample = matrix(0, nrow = n.sample, ncol = p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new = 1/rgamma(1, shape = n.star/2, rate = d.star/2)\n  nu.sample[i] = nu.new\n  phi.new = rmvnorm(1, mean = m, sigma = nu.new * C)\n  phi.sample[i,] = phi.new\n}\n\npar(mfrow=c(1,3))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,2]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\n\n\n\n4.1.3 Model checking by in-sample prediction and interval estimation\nTo check whether the model fits well, we plot the posterior point and interval estimate for each point.\n\n## get in sample prediction\npost.pred.y=function(s){\n  \n  beta.cur=matrix(phi.sample[s,],ncol=1)\n  nu.cur=nu.sample[s]\n  mu.y=t(Fmtx)%*%beta.cur\n  sapply(1:length(mu.y), function(k){rnorm(1,mu.y[k],sqrt(nu.cur))})\n  \n  \n}  \n\ny.post.pred.sample=sapply(1:5000, post.pred.y)\n\n## show the result\nsummary.vec95=function(vec){\n  c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))\n}\n\nsummary.y=apply(y.post.pred.sample,MARGIN=1,summary.vec95)\n\nplot(Y,type='b',xlab='Time',ylab='',pch=16)\nlines(summary.y[2,],type='b',col='grey',lty=2,pch=4)\nlines(summary.y[1,],type='l',col='purple',lty=3)\nlines(summary.y[3,],type='l',col='purple',lty=3)\nlegend(\"topright\",legend=c('Truth','Mean','95% C.I.'),lty=1:3,col=c('black','grey','purple'),\n       horiz = T,pch=c(16,4,NA))\n\n\n\n\n\n\n\n\n\n\n4.1.4 Google search index data\nFor the earthquake data, perform a conjugate Bayesian analysis of AR(2) model to the data. Using priors \\(\\mathbf{m}_{0} = \\begin{bmatrix} 0 & 0\\end{bmatrix}^{\\top}\\), \\(\\mathbf{C}_{0} = 10 \\mathbf{I}_{2}\\), and \\(n_{0} = d_{0} = 0.02\\). Simulate 5000 posterior samples of \\(\\nu\\), \\(\\phi_{1}\\), and \\(\\phi_{2}\\) and use these 5000 posterior samples to perform posterior inference.\nWhat is the posterior mean estimate of two AR coefficients and the variance parameter for the noise term?\n\n## read data, you need to make sure the data file is in your current working directory \ncovid.dat &lt;- read.delim(\"./data/GoogleSearchIndex.txt\")\ncovid.dat$Week=as.Date(as.character(covid.dat$Week),format = \"%Y-%m-%d\")\n\ny.dat=covid.dat$covid[1:57] ## this is the training data\ny.new=covid.dat$covid[58:60] ## this is the test data\n\nplot.ts(\n  y.dat,\n  ylab = expression(italic(y)[italic(t)]),\n  xlab = expression(italic(t)),\n  main = \"\"\n)\n\n\n\n\n\n\n\n\n\n\n4.1.5 Posterior inference\nWe sample 5000 sets of (\\(\\phi_{1}\\), \\(\\phi_{2}\\), \\(\\nu\\)) from their marginal posterior distributions and plot them.\n\nlibrary(mvtnorm)\n\n## set up\np = 2  ## order of AR process\nn.all = length(y.dat) ## T, total number of data\n\nY = matrix(y.dat[3:n.all], ncol = 1)\nFmtx = matrix(c(y.dat[2:(n.all-1)], y.dat[1:(n.all-2)]), nrow=p, byrow=TRUE)\nn = length(Y)\n\n## define the priors\nm0 = matrix(rep(0,p),ncol=1)\nC0 = diag(p)\nn0 = 0.02\nd0 = 0.02\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\n\n## sample from joint posterior distribution\nn.sample = 5000\n\nnu.sample = rep(0, n.sample)\nphi.sample = matrix(0, nrow = n.sample, ncol = p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new = 1/rgamma(1, shape = n.star/2, rate = d.star/2)\n  nu.sample[i] = nu.new\n  phi.new = rmvnorm(1, mean = m, sigma = nu.new * C)\n  phi.sample[i,] = phi.new\n}\n\npar(mfrow=c(1,3))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main=\"\",ylim=c(0,6.4))\nlines(density(phi.sample[,2]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\n\n\n\n4.1.6 Model checking by in-sample prediction and interval estimation\nTo check whether the model fits well, we plot the posterior point and interval estimate for each point.\n\n## get in sample prediction\npost.pred.y=function(s){\n  \n  beta.cur=matrix(phi.sample[s,],ncol=1)\n  nu.cur=nu.sample[s]\n  mu.y=t(Fmtx)%*%beta.cur\n  sapply(1:length(mu.y), function(k){rnorm(1,mu.y[k],sqrt(nu.cur))})\n  \n  \n}  \n\ny.post.pred.sample=sapply(1:5000, post.pred.y)\n\n## show the result\nsummary.vec95=function(vec){\n  c(unname(quantile(vec,0.025)),mean(vec),unname(quantile(vec,0.975)))\n}\n\nsummary.y=apply(y.post.pred.sample,MARGIN=1,summary.vec95)\n\nplot(Y,type='b',xlab='Time',ylab='',pch=16)\nlines(summary.y[2,],type='b',col='grey',lty=2,pch=4)\nlines(summary.y[1,],type='l',col='purple',lty=3)\nlines(summary.y[3,],type='l',col='purple',lty=3)\nlegend(\"topright\",legend=c('Truth','Mean','95% C.I.'),lty=1:3,col=c('black','grey','purple'),\n       horiz = T,pch=c(16,4,NA))",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>AR model fitting example</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/04_model_selection.html",
    "href": "courses/C05 Capstone project/04_model_selection.html",
    "title": "5  Model Selection",
    "section": "",
    "text": "5.1 Model selection criteria\nTo fit our AR model to some real data we need to determine the order of the AR process. One possible way is to repeat the analysis for different values of model order \\(p\\) and choose the best model based on some criteria.\nThe two most widely known criteria are the so-called Akaike information criteria (AIC) and the Bayesian information criteria (BIC).\nHow do we select the order of an AR process using AIC or BIC?\nSuppose we still have data \\(y_{1:T}\\).\nAs we are comparing models with different numbers of parameters, we have to do so based on a common sample size. Thus, we fix a maximum order \\(p^{*}\\), and consider \\(p &lt; p^{*}\\). When comparing models of various orders \\(p\\) we do so with \\(n = T - p^{*}\\)\nFor any order \\(p &lt; p^{*}\\), the AR(\\(p\\)) model can be written as: \\[\n\\mathbf{y} = \\mathbf{F}^{\\top} \\boldsymbol{\\phi} + \\boldsymbol{\\epsilon}\n\\] with \\(\\mathbf{y} = \\begin{bmatrix}y_{p^{*}} & \\ldots & y_{T}\\end{bmatrix}^{\\top}\\) and the design matrix: \\[\n\\mathbf{F} = \\begin{bmatrix}\ny_{p^{*}} & y_{p^{*}+1} & \\cdots & y_{T-1} \\\\\ny_{p^{*}-1} & y_{p^{*}1} & \\cdots & y_{T-2} \\\\\ny_{p^{*}-p+1} & y_{p^{*}-p+2} & \\cdots & y_{T-p} \\\\\n\\end{bmatrix} \\in \\mathbb{R}^{p \\times n}\n\\] and the AR coefficients are just: \\[\\boldsymbol{\\phi} = \\begin{bmatrix}\\phi_{1} & \\phi_{2} & \\cdots \\phi_{p}\\end{bmatrix}^{\\top}\\] and \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N\\left(0,\\nu \\mathbf{I}_{n}\\right)\\).\nThen based on our notation the conditional likelihood is: \\[\n\\begin{align}\nf(y_{(p^{*}+1):T} \\mid y_{1:p^{*}}, \\boldsymbol{\\phi}, \\nu) &= \\mathcal{N}\\left(\\mathbf{F}^{\\top} \\boldsymbol{\\phi}, \\nu \\mathbf{I}_{n}\\right) \\\\\n&= (2 \\pi \\nu)^{1/2} \\exp\\left[-\\frac{(\\mathbf{y} - \\mathbf{F}^{\\top} \\boldsymbol{\\phi})^{\\top} (\\mathbf{y} - \\mathbf{F}^{\\top} \\boldsymbol{\\phi})}{2 \\nu}\\right]\n\\end{align}\n\\]\nNow we can use multivariate linear regression theory to define: \\[\n\\hat{\\boldsymbol{\\phi}} = \\left( \\mathbf{F} \\mathbf{F}^{\\top} \\right)^{-1} \\mathbf{F} \\mathbf{y}\n\\] Then we can expand: \\[\n\\begin{align}\n(\\mathbf{y} - \\mathbf{F}^{\\top} \\boldsymbol{\\phi})^{\\top} (\\mathbf{y} - \\mathbf{F}^{\\top} \\boldsymbol{\\phi}) &= (\\mathbf{y} - \\mathbf{F}^{\\top} \\hat{\\boldsymbol{\\phi}} + \\mathbf{F}^{\\top} \\hat{\\boldsymbol{\\phi}} - \\mathbf{F}^{\\top} \\boldsymbol{\\phi})^{\\top} (\\mathbf{y} - \\mathbf{F}^{\\top} \\hat{\\boldsymbol{\\phi}} + \\mathbf{F}^{\\top} \\hat{\\boldsymbol{\\phi}} - \\mathbf{F}^{\\top} \\boldsymbol{\\phi}) \\\\\n&= \\underbrace{(\\mathbf{y} - \\mathbf{F}^{\\top} \\hat{\\boldsymbol{\\phi}})^{\\top} (\\mathbf{y} - \\mathbf{F}^{\\top} \\hat{\\boldsymbol{\\phi}})}_{\\mathbf{R}} + (\\hat{\\boldsymbol{\\phi}} - \\boldsymbol{\\phi}) \\mathbf{F} \\mathbf{F}^{\\top} (\\hat{\\boldsymbol{\\phi}} - \\boldsymbol{\\phi})\n\\end{align}\n\\] The definitions of the AIC and BIC are then given by: \\[\n\\begin{align}\n\\text{AIC} &= 2 p + n \\log\\left(\\frac{R}{n - p}\\right) \\\\\n\\text{BIC} &= \\log(n) \\cdot p + n \\log\\left(\\frac{R}{n - p}\\right)\n\\end{align}\n\\]\nValues of \\(p\\) leading too small AIC and BIC values are taken as indicative of relatively good model fits. We’ve seen a ton of AR models so explored.\nLarger values of \\(p\\) will tend to give smaller various estimates which decrease this good-of-fit term in both expressions. But this decrease is penalized of parameter dimensions by the first term.\nBIC tends to choose simpler models than AIC.\nWe simulate an AR(2) process and implement the AIC and BIC criteria to check if the best model selected has order 2.\n## Simulate data\nset.seed(1)\nAR.model = list(order = c(2, 0, 0), ar = c(0.5, 0.4))\ny.sample = arima.sim(n = 100, model = AR.model, sd = 0.1)\nplot(y.sample, type = \"l\", xlab = \"time\", ylab = \"\")\nWe check the ACF and PACF:\npar(mfrow = c(1, 2))\nacf(y.sample, main = \"\", xlab = \"Lag\")\npacf(y.sample, main = \"\", xlab = \"Lag\")\nWe now set the maximum AR order, \\(p^{*} = 15\\), and since \\(T = 100\\), we use the last \\(T - p = 85\\) observations for the analysis. We plot the AIC and BIC for different values of \\(p\\):\nn.all = length(y.sample)\np.star = 15\n\nY = matrix(y.sample[(p.star + 1):n.all], ncol = 1)\nsample.all = matrix(y.sample, ncol = 1)\nn = length(Y)\np = seq(1, p.star, by = 1)\n\ndesign.mtx = function(p_cur) {\n  Fmtx = matrix(0, ncol = n, nrow = p_cur)\n  for (i in 1:p_cur) {\n    start.y = p.star + 1 - i\n    end.y = start.y + n - 1\n    Fmtx[i,] = sample.all[start.y : end.y, 1]\n  }\n  return(Fmtx)\n}\n\ncriteria.ar = function(p_cur) {\n  Fmtx = design.mtx(p_cur)\n  beta.hat=chol2inv(chol(Fmtx%*%t(Fmtx)))%*%Fmtx%*%Y\n  R=t(Y-t(Fmtx)%*%beta.hat)%*%(Y-t(Fmtx)%*%beta.hat)\n  sp.square=R/(n-p_cur)\n  aic=2*p_cur+n*log(sp.square)\n  bic=log(n)*p_cur+n*log(sp.square)\n  result=c(aic,bic)\n  return(result)\n}\n\ncriteria = sapply(p, criteria.ar)\n\nplot(p, criteria[1,], type = \"p\", pch = \"a\", col = \"red\", xlab = \"AR order p\", ylab = \"Criterion\", main=\"\", ylim = c(min(criteria) - 10, max(criteria) + 10))\npoints(p, criteria[2,], pch = \"b\", col = \"blue\")",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/04_model_selection.html#deviance-information-criterion",
    "href": "courses/C05 Capstone project/04_model_selection.html#deviance-information-criterion",
    "title": "5  Model Selection",
    "section": "5.2 Deviance Information Criterion",
    "text": "5.2 Deviance Information Criterion\nWe will now talk about the deviance information criterion (DIC). The DIC is a somewhat Bayesian version of the AIC. We will use DIC later to determine the number of components when we have introduced the mixture AR model for time series.\nSuppose we have some data \\(y_{1:T}\\) that is generated from some distribution. For the AR model this takes the form: \\[\n\\begin{align}\nY &\\sim p(y \\mid \\theta) \\\\\n&\\sim \\mathcal{N}\\left( y \\mid \\mathbf{F}^{\\top} \\boldsymbol{\\phi}, \\nu \\mathbf{I}\\right)\n\\end{align}\n\\]\nThe general formula for calculating the DIC arises from the model estimation of expected log density, defined as: \\[\n\\hat{\\text{elpd}}_{\\text{DIC}} = \\log p(y \\mid \\hat{\\theta}_{\\text{Bayes}}) - p_{\\text{DIC}}\n\\] with:\n\n\\(\\hat{\\theta}_{\\text{Bayes}} = \\mathbb{E}(\\theta \\mid y)\\), i.e. the posterior mean of the parameters,\n\\(p_{\\text{DIC}} = 2 (\\log p(y \\mid \\hat{\\theta}_{\\text{Bayes}}) - \\mathbb{E}_{\\text{post}} \\log p(y \\mid \\theta))\\), i.e. the effective number of parameters\n\nThis expectation is an average of \\(\\theta\\) or a posterior distribution, for which normally you do not have a closed form solution. However, we can compute it in a Monte Carlo way. That is, we use posterior samples of parameters \\(\\theta\\) to calculate it. Suppose the posterior sample of model parameters \\(\\theta\\) is denoted as \\(\\theta^{(s)}\\) for \\(s = 1, \\ldots, S\\), then this computed \\(p_{\\text{DIC}}\\) is: \\[\np_{\\text{DIC}} = 2 (\\log p(y \\mid \\hat{\\theta}_{\\text{Bayes}}) - \\frac{1}{S} \\sum_{s=1}^{S} \\log p(y \\mid \\theta^{(s)}))\n\\]\nThe actual quantity, called DIC, is defined in terms of the deviance, rather than the log predictive density.\nThat is: \\[\n\\text{DIC} = -2 \\log p(y \\mid \\hat{\\theta}_{\\text{Bayes}}) + 2 p_{\\text{DIC}}\n\\] ### Get posterior samples\n\nlibrary(mvtnorm)\n\nn.all=length(y.sample)\np=2\nm0=matrix(rep(0,p),ncol=1)\nC0=diag(p)\nn0=2\nd0=2\n\nY=matrix(y.sample[3:n.all],ncol=1)\nFmtx=matrix(c(y.sample[2:(n.all-1)],y.sample[1:(n.all-2)]),nrow=p,byrow=TRUE)\nn=length(Y)\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\nn.sample=5000\n\nnu.sample=rep(0,n.sample)\nphi.sample=matrix(0,nrow=n.sample,ncol=p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)\n  nu.sample[i]=nu.new\n  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)\n  phi.sample[i,]=phi.new\n}\n\npar(mfrow=c(1,3))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\")\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main=\"\")\nlines(density(phi.sample[,2]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\n\n\n5.2.1 Calculate DIC\n\ncal_log_likelihood=function(phi,nu){\n  mu.y=t(Fmtx)%*%phi\n  log.lik=sapply(\n    1:length(mu.y), \n    function(k){\n      dnorm(Y[k,1],mu.y[k],sqrt(nu),log=TRUE)\n    }\n  )\n  sum(log.lik)\n}\n\nphi.bayes=colMeans(phi.sample)\nnu.bayes=mean(nu.sample)\n\nlog.lik.bayes=cal_log_likelihood(phi.bayes,nu.bayes)\n\npost.log.lik=sapply(\n  1:5000, \n  function(k){\n    cal_log_likelihood(phi.sample[k,],nu.sample[k])\n  }\n)\nE.post.log.lik=mean(post.log.lik)\n\np_DIC=2*(log.lik.bayes-E.post.log.lik)\nDIC=-2*log.lik.bayes+2*p_DIC\nDIC\n\n[1] -122.9988",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/99_capstone_project.html",
    "href": "courses/C05 Capstone project/99_capstone_project.html",
    "title": "6  Capstone Project",
    "section": "",
    "text": "6.1 Earthquakes data",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Capstone Project</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/99_capstone_project.html#earthquakes-data",
    "href": "courses/C05 Capstone project/99_capstone_project.html#earthquakes-data",
    "title": "6  Capstone Project",
    "section": "",
    "text": "6.1.1 Get the data\n\nearthquakes.dat &lt;- read.delim(\"./data/earthquakes.txt\")\nearthquakes.dat$Quakes = as.numeric(earthquakes.dat$Quakes)\n\ny.dat=earthquakes.dat$Quakes[1:100] ## this is the training data\ny.new=earthquakes.dat$Quakes[101:103] ## this is the test data\n\nplot.ts(\n  y.dat,\n  ylab = expression(italic(y)[italic(t)]),\n  xlab = expression(italic(t)),\n  main = \"\"\n)\n\n\n\n\n\n\n\n\nWe check the ACF and PACF:\n\npar(mfrow = c(1, 2))\nacf(y.dat, main = \"\", xlab = \"Lag\")\npacf(y.dat, main = \"\", xlab = \"Lag\")\n\n\n\n\n\n\n\n\n\n\n6.1.2 Estimate the model order\nWe now set the maximum AR order, \\(p^{*} = 10\\), and since \\(T = 100\\), we use the last \\(T - p = 90\\) observations for the analysis. We plot the AIC and BIC for different values of \\(p\\):\n\nn.all = length(y.dat)\np.star = 10\n\nY = matrix(y.dat[(p.star + 1):n.all], ncol = 1)\nsample.all = matrix(y.dat, ncol = 1)\nn = length(Y)\np = seq(1, p.star, by = 1)\n\ndesign.mtx = function(p_cur) {\n  Fmtx = matrix(0, ncol = n, nrow = p_cur)\n  for (i in 1:p_cur) {\n    start.y = p.star + 1 - i\n    end.y = start.y + n - 1\n    Fmtx[i,] = sample.all[start.y : end.y, 1]\n  }\n  return(Fmtx)\n}\n\ncriteria.ar = function(p_cur) {\n  Fmtx = design.mtx(p_cur)\n  beta.hat=chol2inv(chol(Fmtx%*%t(Fmtx)))%*%Fmtx%*%Y\n  R=t(Y-t(Fmtx)%*%beta.hat)%*%(Y-t(Fmtx)%*%beta.hat)\n  sp.square=R/(n-p_cur)\n  aic=2*p_cur+n*log(sp.square)\n  bic=log(n)*p_cur+n*log(sp.square)\n  result=c(aic,bic)\n  return(result)\n}\n\ncriteria = sapply(p, criteria.ar)\n\nplot(p, criteria[1,], type = \"p\", pch = \"a\", col = \"red\", xlab = \"AR order p\", ylab = \"Criterion\", main=\"\", ylim = c(min(criteria) - 10, max(criteria) + 10))\npoints(p, criteria[2,], pch = \"b\", col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n6.1.3 Get posterior samples\n\nlibrary(mvtnorm)\n\nn.all=length(y.dat)\np=3\nm0=matrix(rep(0,p),ncol=1)\nC0=10 * diag(p)\nn0=0.02\nd0=0.02\n\nY=matrix(y.dat[(p+1):n.all], ncol=1)\nFmtx=matrix(\n  c(\n    y.dat[3:(n.all-1)],\n    y.dat[2:(n.all-2)],\n    y.dat[1:(n.all-3)]\n  ),\n  nrow=p,byrow=TRUE\n)\nn=length(Y)\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\nn.sample=5000\n\nnu.sample=rep(0,n.sample)\nphi.sample=matrix(0,nrow=n.sample,ncol=p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)\n  nu.sample[i]=nu.new\n  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)\n  phi.sample[i,]=phi.new\n}\n\npar(mfrow=c(1,4))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\")\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(phi.sample[,2],freq=FALSE,xlab=expression(phi[2]),main=\"\")\nlines(density(phi.sample[,2]),type='l',col='red')\nhist(phi.sample[,3],freq=FALSE,xlab=expression(phi[3]),main=\"\")\nlines(density(phi.sample[,3]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\n\n\n\n6.1.4 Calculate DIC\n\ncal_log_likelihood=function(phi,nu){\n  mu.y=t(Fmtx)%*%phi\n  log.lik=sapply(\n    1:length(mu.y), \n    function(k){\n      dnorm(Y[k,1],mu.y[k],sqrt(nu),log=TRUE)\n    }\n  )\n  sum(log.lik)\n}\n\nphi.bayes=colMeans(phi.sample)\nnu.bayes=mean(nu.sample)\n\nlog.lik.bayes=cal_log_likelihood(phi.bayes,nu.bayes)\n\npost.log.lik=sapply(\n  1:5000, \n  function(k){\n    cal_log_likelihood(phi.sample[k,],nu.sample[k])\n  }\n)\nE.post.log.lik=mean(post.log.lik)\n\np_DIC=2*(log.lik.bayes-E.post.log.lik)\nDIC=-2*log.lik.bayes+2*p_DIC\nDIC\n\n[1] 556.901",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Capstone Project</span>"
    ]
  },
  {
    "objectID": "courses/C05 Capstone project/99_capstone_project.html#google-search-index-data",
    "href": "courses/C05 Capstone project/99_capstone_project.html#google-search-index-data",
    "title": "6  Capstone Project",
    "section": "6.2 Google search index data",
    "text": "6.2 Google search index data\n\n6.2.1 Get the data\n\ncovid.dat &lt;- read.delim(\"./data/GoogleSearchIndex.txt\")\ncovid.dat$Week=as.Date(as.character(covid.dat$Week),format = \"%Y-%m-%d\")\n\ny.dat=covid.dat$covid[1:57] ## this is the training data\ny.new=covid.dat$covid[58:60] ## this is the test data\n\nplot.ts(\n  y.dat,\n  ylab = expression(italic(y)[italic(t)]),\n  xlab = expression(italic(t)),\n  main = \"\"\n)\n\n\n\n\n\n\n\n\nWe check the ACF and PACF:\n\npar(mfrow = c(1, 2))\nacf(y.dat, main = \"\", xlab = \"Lag\")\npacf(y.dat, main = \"\", xlab = \"Lag\")\n\n\n\n\n\n\n\n\n\n\n6.2.2 Estimate the model order\nWe now set the maximum AR order, \\(p^{*} = 10\\), and since \\(T = 57\\), we use the last \\(T - p = 47\\) observations for the analysis. We plot the AIC and BIC for different values of \\(p\\):\n\nn.all = length(y.dat)\np.star = 10\n\nY = matrix(y.dat[(p.star + 1):n.all], ncol = 1)\nsample.all = matrix(y.dat, ncol = 1)\nn = length(Y)\np = seq(1, p.star, by = 1)\n\n## The functions are already defined above, \n## no need to repeat ourselves here!\n\n# design.mtx = function(p_cur) {...}\n\n# criteria.ar = function(p_cur) {...}\n\ncriteria = sapply(p, criteria.ar)\n\nplot(p, criteria[1,], type = \"p\", pch = \"a\", col = \"red\", xlab = \"AR order p\", ylab = \"Criterion\", main=\"\", ylim = c(min(criteria) - 10, max(criteria) + 10))\npoints(p, criteria[2,], pch = \"b\", col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n6.2.3 Get posterior samples\n\nlibrary(mvtnorm)\n\nn.all=length(y.dat)\np=1\nm0=matrix(rep(0,p),ncol=1)\nC0=10\nn0=0.02\nd0=0.02\n\nY=matrix(y.dat[(p+1):n.all], ncol=1)\nFmtx=matrix(\n  c(\n    y.dat[1:(n.all-1)]\n  ),\n  nrow=p,byrow=TRUE\n)\nn=length(Y)\n\ne=Y-t(Fmtx)%*%m0\nQ=t(Fmtx)%*%C0%*%Fmtx+diag(n)\nQ.inv=chol2inv(chol(Q))\nA=C0%*%Fmtx%*%Q.inv\nm=m0+A%*%e\nC=C0-A%*%Q%*%t(A)\nn.star=n+n0\nd.star=t(Y-t(Fmtx)%*%m0)%*%Q.inv%*%(Y-t(Fmtx)%*%m0)+d0\n\nn.sample=5000\n\nnu.sample=rep(0,n.sample)\nphi.sample=matrix(0,nrow=n.sample,ncol=p)\n\nfor (i in 1:n.sample) {\n  set.seed(i)\n  nu.new=1/rgamma(1,shape=n.star/2,rate=d.star/2)\n  nu.sample[i]=nu.new\n  phi.new=rmvnorm(1,mean=m,sigma=nu.new*C)\n  phi.sample[i,]=phi.new\n}\n\npar(mfrow=c(1,2))\nhist(phi.sample[,1],freq=FALSE,xlab=expression(phi[1]),main=\"\")\nlines(density(phi.sample[,1]),type='l',col='red')\nhist(nu.sample,freq=FALSE,xlab=expression(nu),main=\"\")\nlines(density(nu.sample),type='l',col='red')\n\n\n\n\n\n\n\n\n\n\n6.2.4 Calculate DIC\n\ncal_log_likelihood=function(phi,nu){\n  mu.y=t(Fmtx)%*%phi\n  log.lik=sapply(\n    1:length(mu.y), \n    function(k){\n      dnorm(Y[k,1],mu.y[k],sqrt(nu),log=TRUE)\n    }\n  )\n  sum(log.lik)\n}\n\nphi.bayes=colMeans(phi.sample)\nnu.bayes=mean(nu.sample)\n\nlog.lik.bayes=cal_log_likelihood(phi.bayes,nu.bayes)\n\npost.log.lik=sapply(\n  1:5000, \n  function(k){\n    cal_log_likelihood(phi.sample[k,],nu.sample[k])\n  }\n)\nE.post.log.lik=mean(post.log.lik)\n\np_DIC=2*(log.lik.bayes-E.post.log.lik)\nDIC=-2*log.lik.bayes+2*p_DIC\nDIC\n\n[1] 411.4288",
    "crumbs": [
      "Capstone project",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Capstone Project</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]